{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk assesment for an Insurance company for Vehicle loans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem Statement: An Insurance company wants to asses the risk of their existing customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Collection: We have got the data from a trusted source where we have previously whipped the data into shape.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading packages\n",
    "import numpy as np  #Numpy provides a large set of numeric datatypes that you can use to construct arrays.\n",
    "import pandas as pd #pandas DataFrame - allows data manipulation functions with numpy record arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_data = pd.read_excel(r'C:\\Users\\ghreddy\\Desktop\\dxc\\CustomerRiskProfile_Dataset_IIDT.xlsx') #load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_data.shape #checking the size of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_data.head() #to check the attributes in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns, to spot anomalies, to test hypothesis and to check assumptions with the help of summary statistics and graphical representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_data.dtypes #checking the datatypes of each attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_data.isnull().sum() #checking for null values in the data.If we find any missing values we have to impute the missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for unique value records for each categorical attribute.\n",
    "cat_var=[]\n",
    "for i in risk_data.columns:\n",
    "    if risk_data.dtypes[i]=='object':\n",
    "        count = risk_data[i].value_counts()\n",
    "        print(count)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt #Matplotlib is a plotting library for numerical mathematics & provides visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_data['Gender'].value_counts() #to find the unique value records in the attribute 'Gender'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bivariate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of determining the empirical relationship between two(Dependent and Independent) variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(risk_data['Risk Profile'],risk_data['Gender']).plot.bar() #gives us the bar plot for Gender contribution to risk profile.\n",
    "pd.crosstab(risk_data['Risk Profile'],risk_data['Gender'],margins=True) #Gender contribution to risk profile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost 63% of our customers are associated with medium risk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Driving frequency contribution to Risk profile.\n",
    "pd.crosstab(risk_data['Risk Profile'],risk_data['Driving Frequency'],margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest number of customers are at medium risk who drive occasionally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Driving purpose contribution to Risk profile.\n",
    "pd.crosstab(risk_data['Risk Profile'],risk_data['Driving Purpose'],margins=True),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly we can find that the customers who took loans for racing purpose are least at low risk and for the customers who are at high risk, racing tops the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Driving experience contribution to Risk profile.\n",
    "pd.crosstab(risk_data['Risk Profile'],risk_data['Driving Experience'],margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The customers who have less driving experience are classified as high risk customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Age of vehicle contribution to Risk profile.\n",
    "pd.crosstab(risk_data['Risk Profile'],risk_data['Age of Vehicle'],margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building features for each label while filtering to make valid features.\n",
    "\n",
    "\n",
    "Encoding the categorical variables for fitting the model.\n",
    "We can encode the categorical variables in various procedures as OneHotEncoding or LabelEncoder but we have done it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns #Seaborn is a Python data visualization library based on matplotlib. \n",
    "#It provides a high-level interface for drawing attractive and informative statistical graphics.\n",
    "sns.distplot(risk_data['Age'], bins=20) #Checking the distribution of age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The StandardScaler assumes data is normally distributed and will scale them such that the distribution\n",
    "#is centred around 0, with a standard deviation of 1.\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "sc = StandardScaler()\n",
    "risk_data['Age']=sc.fit_transform(risk_data[['Age']]) #fitting the standard scaler on Age column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_data['Age'].head() #Checking for the scaled values of Age column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(risk_data['Age'], bins=20), #Checking the distribution of age after scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_data['Gender']=risk_data['Gender'].replace({'M':0,'F':1}) #Manually replacing the categories in the gender column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually replacing the categories in the education column.\n",
    "risk_data['Education'] = risk_data['Education'].replace({'Primary School':0,'College':1,'Secondary School':2,'None':3,'Bachelors':4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually replacing the categories in the vehicle condition column.\n",
    "risk_data['Vehicle Condition']=risk_data['Vehicle Condition'].replace({'Ex-demonstrator':0,'Classic/Vintage':1,'New':2,'Used':3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(risk_data['Market Value'], bins=20) #Checking the distribution of market value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming the market value column as it has large numeric values.\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "sc = StandardScaler()\n",
    "#fitting the standard scaler on Market value column to standardize its values.\n",
    "risk_data['Market Value']=sc.fit_transform(risk_data[['Market Value']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_data['Market Value'].head() #Checking for the scaled values of market value column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually replacing the categories in the luxury category column.\n",
    "risk_data['Luxury Category']=risk_data['Luxury Category'].replace({'Semi Luxury':0,'Luxury':1,'Compact':2,'Economy':3,'Intermediate':4,'Super Luxury':5,'Mini':6,'Executive':7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually replacing the categories in the driving time column.\n",
    "risk_data['Driving Time']=risk_data['Driving Time'].replace({'Day Time':0,'Any Time':1,'Night Time':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually replacing the categories in the driving frequency column.\n",
    "risk_data['Driving Frequency']=risk_data['Driving Frequency'].replace({'Occasionaly':0,'Daily':1,'Weekly':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually replacing the categories in the driving purpose column.\n",
    "risk_data['Driving Purpose'] = risk_data['Driving Purpose'].replace({'Business':0,'Personal':1,'Racing':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually replacing the categories in the risk profile column.\n",
    "risk_data['Risk Profile']=risk_data['Risk Profile'].replace({\"Medium\":0,'High':1,'Low':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k#since we are unable to handle huge number of unique values manually we can do LabelEncoding.\n",
    "columns=['Location Name']\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "for i in columns:\n",
    "    risk_data[i] = le.fit_transform(risk_data[i])\n",
    "risk_data['Location Name'] = risk_data[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_data['Location Name'].head() #checking the values in the location name column after encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually replacing the categories in the road type column.\n",
    "risk_data['Road Type'] = risk_data['Road Type'].replace({'Suburban':0,'Urban':1,'Highway':2,'Extra-urban High Density':3,'Extra-urban Low Density':4,'Rural':5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually replacing the categories in the topography column.\n",
    "risk_data['Topography']=risk_data['Topography'].replace({'Plains':0,'Hills':1,'Rocky area':2,'Mountains':3,'Valleys':4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To plot correlation graph we use seaborn package.\n",
    "#we get the correlation of each column compared to the other one.\n",
    "corr = risk_data.corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the size, shape and other parameters to plot the graph.\n",
    "plt.figure(figsize=(10,10))\n",
    "cmap = sns.diverging_palette(220,10,as_cmap=True) #220 is for height and 10 is for width\n",
    "sns.heatmap(corr,xticklabels=corr.columns.values,yticklabels=corr.columns.values,cmap=cmap,vmax=.3,center=0,\n",
    "            square=True,linewidths=0.5,cbar_kws={'shrink':.82})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building and Test with accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing packages.\n",
    "#In data mining, a decision tree describes data (but the resulting classification tree can be an input for decision making).\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividing the data into two parts as the dependant and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependant variables\n",
    "x_train = risk_data.drop(['Risk Profile'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#independant variables\n",
    "y_train = risk_data['Risk Profile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape #check the shape of training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the header of the data after feature engineering.\n",
    "risk_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the training dataset as train and test.\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(x_train,y_train,test_size=0.3,random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for train data and test data.\n",
    "X_train.shape,Y_train.shape,X_test.shape,Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiating the decision tree classifier.\n",
    "tree_model = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting the decision tree model\n",
    "tree_model.fit(X_train,Y_train)\n",
    "#class_weight:Weights associated with classes in the form\n",
    "#criterion :The function to measure the quality of a split.\n",
    "#max_depth :The maximum depth of the tree.\n",
    "#min_samples_split :The minimum number of samples required to split an internal node.\n",
    "#min_samples_leaf :The minimum number of samples required to be at a leaf node.\n",
    "#splitter :The strategy used to choose the split at each node. Supported strategies are “best” to choose the best split and \n",
    "#           “random” to choose the best random split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting the model on test data.\n",
    "predict_tree = tree_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the accuracy for decision tree model.\n",
    "accuracy_score(Y_test,predict_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the Cohen's kappa score for the y_test by the predicted values of x_test.\n",
    "#Cohen's kappa coefficient (κ) is a statistic which measures inter-rater agreement test and train items.\n",
    "cohen_kappa_score(Y_test,predict_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature selection: using feat map we will find feature importances and eliminate the columns based on percentiles we got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the contribution of each variable in the decision tree model.\n",
    "#Feature selection is usually used as a pre-processing step before doing the actual learning. \n",
    "feat_map = pd.Series(tree_model.feature_importances_,index=X_train.columns).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we conclude that driving frequency, vehicle condition, driving purpose, luxury cateogery and gender can be deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for accuracy after dropping the low performance attibutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the low performance attributes and assigning the dataframe to another variable for future use. \n",
    "risk_data_new = risk_data.drop(['Driving Frequency','Vehicle Condition','Driving Purpose','Luxury Category','Gender'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_data_new.head() #Checking for the data after dropping the low performance attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#division of dependant and independant variables.\n",
    "x_train_new = risk_data_new.drop(['Risk Profile'],axis=1)\n",
    "y_train_new = risk_data['Risk Profile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new,X_test_new,Y_train_new,Y_test_new = train_test_split(x_train_new,y_train_new,test_size=0.3,random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model_new = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model_new.fit(X_train_new,Y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_tree_new = tree_model_new.predict(X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(Y_test_new,predict_tree_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the Cohen's kappa score for the y_test by the predicted values of x_test.\n",
    "cohen_kappa_score(Y_test_new,predict_tree_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lets try to find accuracy by dropping the location column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new1 = risk_data.drop(['Risk Profile','Location Name'],axis=1) #defining new train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_new1 = risk_data['Risk Profile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1,X_test1,Y_train1,Y_test1 = train_test_split(x_train_new1,y_train_new1,test_size=0.3,random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model1 = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model1.fit(X_train1,Y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_1 = tree_model1.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(predict_1,Y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the Cohen's kappa score for the y_test by the predicted values of x_test.\n",
    "cohen_kappa_score(Y_test1,predict_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing the cohen_kappa_score before and after dropping the 'location name' we can understand that location is an important attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NaiveBayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption of conditional independence between every pair of features given the value of the class variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required packages.\n",
    "#GaussianNB implements the Gaussian Naive Bayes algorithm for classification. \n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB() #Initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model.\n",
    "nb.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_nb= nb.predict(X_test) #Predicted values for X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(predict_nb,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the Cohen's kappa score for the y_test by the predicted values of x_test.\n",
    "cohen_kappa_score(predict_nb,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes or mean prediction of the individual trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the packages.\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model.\n",
    "rf.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting the values for x_test.\n",
    "predict_rf = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the accuracy for the y_test by the predicted values of x_test. \n",
    "accuracy_score(predict_rf,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the Cohen's kappa score for the y_test by the predicted values of x_test.\n",
    "cohen_kappa_score(predict_rf,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the model on remaining attributes after removing the low performance attributes.\n",
    "rf.fit(X_train_new,Y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting the values for x test.\n",
    "predict_rf_new = rf.predict(X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the accuracy for the y_test by the predicted values of x_test. \n",
    "accuracy_score(predict_rf_new,Y_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the Cohen's kappa score for the y_test by the predicted values of x_test.\n",
    "cohen_kappa_score(predict_rf_new,Y_test_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# we can find the difference between the three classifiers \n",
    "The Cohen's kappa score for DecisionTreeClassifier on complete data gave 73% whereas the RandomForestClassifier gave us 77.7%.\n",
    "After feature selection from caluclation of feature importances from decision tree, we observed an increase in both of these classifications.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
